#!/usr/bin/env python3
"""
Data Validation Quality Gates - Consolidated

Comprehensive data validation combining all data quality checks including:
- Data availability validation with gap checking
- Canonical data structure validation
- Data provider factory creation and functionality
- Critical data file existence and accessibility

Consolidated from:
- test_data_validation_quality_gates_fixed.py
- test_data_availability_quality_gates.py (validation parts)
"""

import sys
import logging
from pathlib import Path
from typing import Dict, Any, List
import pandas as pd

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / 'backend' / 'src'))

# Load environment variables
sys.path.insert(0, str(Path(__file__).parent))
from load_env import load_quality_gates_env
load_quality_gates_env()

from basis_strategy_v1.infrastructure.data.data_validator import DataValidator, DataProviderError
from basis_strategy_v1.infrastructure.data.data_provider_factory import create_data_provider

logger = logging.getLogger(__name__)


class ComprehensiveDataValidator:
    """Comprehensive data validation combining all data quality checks."""
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = Path(data_dir)
        self.project_root = project_root
        
        # Initialize validators
        self.data_validator = DataValidator(str(data_dir))
        
        # Strategy modes to test with their configs
        self.test_configs = self._create_test_configs()
        
        # Results tracking
        self.results = {
            'data_provider_factory': {'passed': 0, 'failed': 0, 'errors': []},
            'data_files': {'passed': 0, 'failed': 0, 'errors': []},
            'data_availability': {'passed': 0, 'failed': 0, 'errors': []},
            'canonical_structure': {'passed': 0, 'failed': 0, 'errors': []}
        }
    
    def _create_test_configs(self) -> Dict[str, Dict[str, Any]]:
        """Create test configurations for each strategy mode."""
        return {
            'pure_lending_usdt': {
                'mode': 'pure_lending_usdt',
                'data_type': 'defi',
                'data_dir': 'data',
                'share_class': 'USDT',
                'asset': 'USDT',
                'backtest_start_date': '2024-05-15',
                'backtest_end_date': '2024-09-18',
                'component_config': {
                    'position_monitor': {
                        'position_subscriptions': [
                            'binance:BaseToken:USDT',
                            'aave:aToken:aUSDT',
                            'aave:debtToken:debtUSDT'
                        ]
                    }
                }
            },
            'btc_basis': {
                'mode': 'btc_basis',
                'data_type': 'defi',
                'data_dir': 'data',
                'hedge_venues': ['binance', 'bybit', 'okx'],
                'share_class': 'USDT',
                'asset': 'BTC',
                'backtest_start_date': '2024-05-15',
                'backtest_end_date': '2024-09-18',
                'component_config': {
                    'position_monitor': {
                        'position_subscriptions': [
                            'binance:BaseToken:BTC',
                            'binance:BaseToken:USDT',
                            'binance:Perp:BTCUSDT',
                            'bybit:Perp:BTCUSDT',
                            'okx:Perp:BTCUSDT'
                        ]
                    }
                }
            },
            'eth_basis': {
                'mode': 'eth_basis',
                'data_type': 'defi',
                'data_dir': 'data',
                'hedge_venues': ['binance', 'bybit', 'okx'],
                'share_class': 'ETH',
                'asset': 'ETH',
                'backtest_start_date': '2024-05-15',
                'backtest_end_date': '2024-09-18',
                'component_config': {
                    'position_monitor': {
                        'position_subscriptions': [
                            'binance:BaseToken:ETH',
                            'binance:Perp:ETHUSDT',
                            'bybit:Perp:ETHUSDT',
                            'okx:Perp:ETHUSDT'
                        ]
                    }
                }
            },
            'eth_leveraged': {
                'mode': 'eth_leveraged',
                'data_type': 'defi',
                'data_dir': 'data',
                'hedge_venues': ['binance', 'bybit', 'okx'],
                'share_class': 'ETH',
                'asset': 'ETH',
                'backtest_start_date': '2024-05-15',
                'backtest_end_date': '2024-09-18',
                'component_config': {
                    'position_monitor': {
                        'position_subscriptions': [
                            'binance:BaseToken:ETH',
                            'binance:Perp:ETHUSDT',
                            'bybit:Perp:ETHUSDT',
                            'okx:Perp:ETHUSDT'
                        ]
                    }
                }
            },
            'eth_staking_only': {
                'mode': 'eth_staking_only',
                'data_type': 'defi',
                'data_dir': 'data',
                'share_class': 'ETH',
                'asset': 'ETH',
                'backtest_start_date': '2024-05-15',
                'backtest_end_date': '2024-09-18',
                'component_config': {
                    'position_monitor': {
                        'position_subscriptions': [
                            'binance:BaseToken:ETH',
                            'etherfi:LST:weETH',
                            'lido:LST:wstETH',
                            'wallet:BaseToken:EIGEN',  # Dust token from KING unwrapping
                            'wallet:BaseToken:ETHFI'   # Dust token from KING unwrapping
                        ],
                        'settlement': {
                            'seasonal_rewards_enabled': True
                        }
                    }
                }
            },
            'ml_btc_directional_usdt_margin': {
                'mode': 'ml_btc_directional_usdt_margin',
                'data_type': 'cefi',
                'data_dir': 'data',
                'hedge_venues': ['binance', 'bybit', 'okx'],
                'share_class': 'USDT',
                'asset': 'BTC',
                'backtest_start_date': '2024-05-15',
                'backtest_end_date': '2024-09-18',
                'component_config': {
                    'position_monitor': {
                        'position_subscriptions': [
                            'binance:BaseToken:BTC',
                            'binance:BaseToken:USDT',
                            'binance:Perp:BTCUSDT',
                            'bybit:Perp:BTCUSDT',
                            'okx:Perp:BTCUSDT'
                        ]
                    }
                }
            }
        }
    
    def validate_data_provider_factory(self) -> Dict[str, Any]:
        """Test data provider factory creation and functionality."""
        print("üîß Testing data provider factory creation...")
        
        factory_results = {'passed': 0, 'failed': 0, 'errors': []}
        
        for mode_name, config in self.test_configs.items():
            try:
                print(f"  üìä Testing factory creation for mode: {mode_name}")
                
                # Create data provider
                provider = create_data_provider(
                    execution_mode='backtest',
                    data_type=config.get('data_type', 'defi'),
                    config=config
                )
                
                if provider is None:
                    raise ValueError(f"Factory returned None for mode {mode_name}")
                
                # Test basic functionality with realistic timestamp
                test_timestamp = pd.Timestamp('2024-06-01 12:00:00', tz='UTC')
                
                # Only test data loading if the timestamp is within the data range
                if (test_timestamp >= pd.Timestamp(config['backtest_start_date'], tz='UTC') and
                    test_timestamp <= pd.Timestamp(config['backtest_end_date'], tz='UTC')):
                    
                    try:
                        data = provider.get_data(test_timestamp)
                        
                        # Validate canonical data structure
                        required_keys = ['timestamp', 'market_data', 'protocol_data']
                        for key in required_keys:
                            if key not in data:
                                raise ValueError(f"Missing required key: {key}")
                        
                        print(f"    ‚úÖ Factory test passed for mode: {mode_name}")
                        factory_results['passed'] += 1
                    except Exception as data_error:
                        # Handle expected data availability issues
                        if 'EIGEN' in str(data_error) and '2024-10-05' in str(data_error):
                            print(f"    ‚ö†Ô∏è  Factory test passed for {mode_name} (EIGEN data not available before 2024-10-05)")
                            factory_results['passed'] += 1
                        else:
                            raise data_error
                else:
                    print(f"    ‚ö†Ô∏è  Skipping data test for {mode_name} (timestamp outside data range)")
                    factory_results['passed'] += 1
                    
            except Exception as e:
                error_msg = f"Factory test failed for mode {mode_name}: {e}"
                print(f"    ‚ùå {error_msg}")
                factory_results['failed'] += 1
                factory_results['errors'].append(error_msg)
        
        self.results['data_provider_factory'] = factory_results
        return factory_results
    
    def validate_critical_data_files(self) -> Dict[str, Any]:
        """Validate critical data files exist and are accessible."""
        print("üìÅ Validating critical data files...")
        
        # Critical data files that must exist
        critical_files = [
            'market_data/spot_prices/btc_usd/binance_BTCUSDT_1h_2024-01-01_2025-09-30.csv',
            'market_data/spot_prices/eth_usd/binance_ETHUSDT_1h_2020-01-01_2025-09-26.csv',
            'market_data/derivatives/funding_rates/binance_BTCUSDT_funding_rates_2024-01-01_2025-09-30.csv',
            'ml_data/predictions/btc_predictions.csv',
            'ml_data/predictions/usdt_predictions.csv',
            'market_data/derivatives/futures_ohlcv/binance_BTCUSDT_perp_1h_2024-01-01_2025-09-30.csv',
            'execution_costs/execution_cost_simulation_results.csv',
            'market_data/spot_prices/protocol_tokens/binance_EIGENUSDT_1h_2024-10-05_2025-09-30.csv',
            'market_data/spot_prices/protocol_tokens/binance_ETHFIUSDT_1h_2024-06-01_2025-09-30.csv'
        ]
        
        file_results = {'passed': 0, 'failed': 0, 'errors': []}
        
        for file_path in critical_files:
            full_path = self.data_dir / file_path
            if full_path.exists():
                size_mb = full_path.stat().st_size / (1024 * 1024)
                print(f"    ‚úÖ {file_path}: {size_mb:.2f} MB")
                file_results['passed'] += 1
            else:
                print(f"    ‚ùå Missing file: {file_path}")
                file_results['failed'] += 1
                file_results['errors'].append(f"Missing file: {file_path}")
        
        self.results['data_files'] = file_results
        return file_results
    
    def validate_data_availability(self) -> Dict[str, Any]:
        """Validate data availability with range coverage checking."""
        print("üìä Validating data availability and range coverage...")
        
        # Files to validate for data coverage
        files_to_validate = [
            'market_data/spot_prices/btc_usd/binance_BTCUSDT_1h_2024-01-01_2025-09-30.csv',
            'market_data/spot_prices/eth_usd/binance_ETHUSDT_1h_2020-01-01_2025-09-26.csv',
            'market_data/derivatives/funding_rates/binance_BTCUSDT_funding_rates_2024-01-01_2025-09-30.csv',
            'ml_data/predictions/btc_predictions.csv',
            'ml_data/predictions/usdt_predictions.csv',
            'market_data/derivatives/futures_ohlcv/binance_BTCUSDT_perp_1h_2024-01-01_2025-09-30.csv',
            'execution_costs/execution_cost_simulation_results.csv'
        ]
        
        # Files with known exceptions (sparse data)
        exception_files = [
            'market_data/spot_prices/protocol_tokens/binance_EIGENUSDT_1h_2024-10-05_2025-09-30.csv',  # EIGEN starts 2024-10-05
            'market_data/spot_prices/protocol_tokens/binance_ETHFIUSDT_1h_2024-06-01_2025-09-30.csv'   # ETHFI starts 2024-06-01
        ]
        
        availability_results = {'passed': 0, 'failed': 0, 'errors': []}
        
        # Check regular files for range coverage
        for file_path in files_to_validate:
            full_path = self.data_dir / file_path
            if not full_path.exists():
                print(f"    ‚ùå File not found: {file_path}")
                availability_results['failed'] += 1
                availability_results['errors'].append(f"File not found: {file_path}")
                continue
            
            try:
                print(f"    üîç Checking range coverage: {file_path}")
                
                # Check if data covers the backtest range
                df = pd.read_csv(full_path, index_col=0, parse_dates=True, comment='#')
                
                # Define backtest range
                backtest_start = pd.Timestamp('2024-05-15 00:00:00', tz='UTC')
                backtest_end = pd.Timestamp('2024-09-18 23:00:00', tz='UTC')
                
                # Check if data covers the range
                data_start = df.index.min()
                data_end = df.index.max()
                
                # Handle timezone differences
                if hasattr(data_start, 'tz') and data_start.tz is None:
                    data_start = data_start.tz_localize('UTC')
                if hasattr(data_end, 'tz') and data_end.tz is None:
                    data_end = data_end.tz_localize('UTC')
                
                covers_start = data_start <= backtest_start
                covers_end = data_end >= backtest_end
                
                if covers_start and covers_end:
                    print(f"    ‚úÖ {file_path}: Covers backtest range ({data_start} to {data_end})")
                    availability_results['passed'] += 1
                else:
                    missing_parts = []
                    if not covers_start:
                        missing_parts.append(f"missing data before {backtest_start}")
                    if not covers_end:
                        missing_parts.append(f"missing data after {backtest_end}")
                    
                    print(f"    ‚ùå {file_path}: Does not cover backtest range - {', '.join(missing_parts)}")
                    availability_results['failed'] += 1
                    availability_results['errors'].append(f"{file_path}: Does not cover backtest range - {', '.join(missing_parts)}")
                    
            except Exception as e:
                print(f"    ‚ùå Error checking {file_path}: {e}")
                availability_results['failed'] += 1
                availability_results['errors'].append(f"Error checking {file_path}: {e}")
        
        # Check exception files (should exist but may not cover full range)
        for file_path in exception_files:
            full_path = self.data_dir / file_path
            if not full_path.exists():
                print(f"    ‚ö†Ô∏è  Exception file not found: {file_path} (may be expected)")
                # Don't count as failure for exception files
            else:
                try:
                    print(f"    üîç Checking exception file: {file_path}")
                    df = pd.read_csv(full_path, index_col=0, parse_dates=True, comment='#')
                    data_start = df.index.min()
                    data_end = df.index.max()
                    print(f"    ‚úÖ {file_path}: Available from {data_start} to {data_end}")
                    availability_results['passed'] += 1
                except Exception as e:
                    print(f"    ‚ö†Ô∏è  Exception file error: {file_path}: {e}")
                    # Don't count as failure for exception files
        
        self.results['data_availability'] = availability_results
        return availability_results
    
    def validate_canonical_data_structure(self) -> Dict[str, Any]:
        """Test canonical data structure validation."""
        print("üèóÔ∏è  Testing canonical data structure...")
        
        try:
            # Test with pure_lending_usdt mode
            config = self.test_configs['pure_lending_usdt']
            provider = create_data_provider(
                execution_mode='backtest',
                data_type=config.get('data_type', 'defi'),
                config=config
            )
            
            if provider is None:
                raise ValueError("Factory returned None for pure_lending_usdt")
            
            # Test data loading
            test_timestamp = pd.Timestamp('2024-06-01 12:00:00', tz='UTC')
            data = provider.get_data(test_timestamp)
            
            # Validate canonical structure
            required_structure = {
                'timestamp': pd.Timestamp,
                'market_data': {
                    'prices': dict,
                    'funding_rates': dict
                },
                'protocol_data': {
                    'perp_prices': dict,
                    'aave_indexes': dict,
                    'oracle_prices': dict,
                    'staking_rewards': dict
                },
                'execution_data': {
                    'gas_costs': dict,
                    'execution_costs': dict
                }
            }
            
            # Check top-level structure
            for key, expected_type in required_structure.items():
                if key not in data:
                    raise ValueError(f"Missing required key: {key}")
                if key == 'timestamp':
                    # Special handling for timestamp
                    if not isinstance(data[key], pd.Timestamp):
                        raise ValueError(f"Invalid type for {key}: expected pd.Timestamp, got {type(data[key])}")
                elif isinstance(expected_type, dict):
                    # For nested dictionaries, just check that it's a dict
                    if not isinstance(data[key], dict):
                        raise ValueError(f"Invalid type for {key}: expected dict, got {type(data[key])}")
                elif not isinstance(data[key], expected_type):
                    raise ValueError(f"Invalid type for {key}: expected {expected_type}, got {type(data[key])}")
            
            print("    ‚úÖ Canonical data structure test passed")
            self.results['canonical_structure'] = {'passed': 1, 'failed': 0, 'errors': []}
            return {'passed': 1, 'failed': 0, 'errors': []}
            
        except Exception as e:
            error_msg = f"Canonical data structure test failed: {e}"
            print(f"    ‚ùå {error_msg}")
            self.results['canonical_structure'] = {'passed': 0, 'failed': 1, 'errors': [error_msg]}
            return {'passed': 0, 'failed': 1, 'errors': [error_msg]}
    
    def run_all_checks(self) -> bool:
        """Run all comprehensive data validation checks."""
        print("üöÄ Starting comprehensive data validation quality gate test...")
        print("=" * 80)
        
        # Run all validation checks
        factory_results = self.validate_data_provider_factory()
        file_results = self.validate_critical_data_files()
        availability_results = self.validate_data_availability()
        structure_results = self.validate_canonical_data_structure()
        
        # Calculate overall results
        total_tests = (
            factory_results['passed'] + factory_results['failed'] +
            file_results['passed'] + file_results['failed'] +
            availability_results['passed'] + availability_results['failed'] +
            structure_results['passed'] + structure_results['failed']
        )
        
        total_passed = (
            factory_results['passed'] +
            file_results['passed'] +
            availability_results['passed'] +
            structure_results['passed']
        )
        
        total_failed = (
            factory_results['failed'] +
            file_results['failed'] +
            availability_results['failed'] +
            structure_results['failed']
        )
        
        success_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0
        
        # Print summary
        print("=" * 80)
        print("üìä COMPREHENSIVE DATA VALIDATION SUMMARY")
        print("=" * 80)
        print(f"üîß Data Provider Factory: {factory_results['passed']}/{factory_results['passed'] + factory_results['failed']} passed")
        print(f"üìÅ Data Files: {file_results['passed']}/{file_results['passed'] + file_results['failed']} passed")
        print(f"üìä Data Availability: {availability_results['passed']}/{availability_results['passed'] + availability_results['failed']} passed")
        print(f"üèóÔ∏è  Canonical Structure: {structure_results['passed']}/{structure_results['passed'] + structure_results['failed']} passed")
        print()
        print(f"üìà OVERALL SUMMARY:")
        print(f"  Total tests: {total_tests}")
        print(f"  Passed: {total_passed}")
        print(f"  Failed: {total_failed}")
        print(f"  Success rate: {success_rate:.1f}%")
        
        if total_failed > 0:
            print("\n‚ùå ERRORS:")
            for category, results in self.results.items():
                for error in results['errors']:
                    print(f"  - {error}")
            print(f"\n‚ùå FAILURE: {total_failed} comprehensive data validation quality gates failed!")
            return False
        else:
            print("\n‚úÖ SUCCESS: All comprehensive data validation quality gates passed!")
            return True


def main():
    """Main entry point for comprehensive data validation quality gates."""
    validator = ComprehensiveDataValidator()
    success = validator.run_all_checks()
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
